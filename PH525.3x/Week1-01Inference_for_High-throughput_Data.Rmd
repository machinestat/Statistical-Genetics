---
title: "Inference for High-throughput Data "

output: pdf_document
---
```{r}
library(rafalib)
```
## Introduction
Supposed we were given high-throughput gene expression data that was measured for several individuals in two populations. We are asked to report which genes have different average expression levels in the two populations. Note that if, instead thousands of genes, we were handed data from just one gene we could simply apply the inference techniques that we have learned before. We could, for example, use a t-test or some other test. Here we review what changes when we consider high-throughput data.

## Thousands of test
In this data we have two groups denoted with 0 and 1:
```{r}
library(GSE5859Subset)
data(GSE5859Subset)
g <- sampleInfo$group
g
```

If we were interested in a particular gene, letâ€™s arbitrarily pick the one on the 25th row, we would simply compute a t-test; assuming the data is well approximated by normal:
```{r}
e <- geneExpression[25,]
mypar2(1,2)
qqnorm(e[g==1])
qqline(e[g==1])
qqnorm(e[g==0])
qqline(e[g==0])
```

The qq-plots show that the data is well approximated by the normal approximation so apply a t-test. The t-test does not find this gene to be statistically significant:
```{r}
t.test(e[g==1],e[g==0])
```

To answer the question for each gene we simply do this for every gene. Here we will define our own function and use apply:
```{r}
myttest <- function(x) t.test(x[g==1],x[g==0],var.equal=TRUE)$p.value
pvals <- apply(geneExpression,1,myttest)
```

We can now see which genes have p-values less than, say, 0.05. For example right away we see that there are `r sum(pvals<0.05)` genes had p-values less than 0.05

However, as we will describe in more detail below, we have to be careful in interpreting this result because we have performed over 8,000 test. Note that if we performed the same procedure on random data, for which the null hypothesis is true for all feature, we obtain the following results:
```{r}
set.seed(1)
m <- nrow(geneExpression)
n <- ncol(geneExpression)
randomData <- matrix(rnorm(n*m),m,n)
nullpvals <- apply(randomData,1,myttest)
sum(nullpvals<0.05)
```

Note that 419 is roughly 0.05*8192 and we will describe the theory that tells us why this prediction works.

## Faster implementation of t-test
Before, we continue, we should note that the above implementation is very inefficient. There are several faster implementations that perform t-test for high throughput data. For example
```{r}
library(genefilter)
results <- rowttests(geneExpression,factor(g))
max(abs(pvals-results$p))
```

`genefilter` is available from the Bioconductor projects and here is how one installs it:
```
source("http://www.bioconductor.org/biocLite.R")
biocLite("genefilter")
```

## Excersi
**Question 1.2.1**

p-values are random variables.

Note that just like the sample average is a random variable because it is based on a random sample, p-values are based on random variables (sample mean, sample standard deviation) so they are also a random variable.

To see this let's see how p-values change when we take different samples. 

```{r, echo=TRUE, cache=TRUE}
set.seed(1)
library(downloader)
url = "https://raw.githubusercontent.com/genomicsclass/dagdata/master/inst/extdata/femaleControlsPopulation.csv"
filename = "femaleControlsPopulation.csv"
if (!file.exists(filename)) download(url,destfile=filename)
population = read.csv(filename)
pvals <- replicate(1000,{
  control = sample(population[,1],12)
  treatment = sample(population[,1],12)
  t.test(treatment,control)$p.val
})
head(pvals)
hist(pvals)
```

Question: What proportion of the p-values is below 0.05?
`r mean(pvals < 0.05)`

What proportion of the p-values is below 0.01?
`r mean(pvals < 0.01)`

**Question 1.2.3**

Assume you are testing the effectiveness of 20 diets on mice weight. For each of the 20 diets you run an experiment with 10 control mice and 10 treated mice. Assume the null hypothesis that the diet has no effect is true for all 20 diets and that mice weights follow a normal distribution with mean 30 grams and a standard deviation of 2 grams, run a Monte Carlo simulation for one of these studies:
```{r}
cases = rnorm(10,30,2)
controls = rnorm(10,30,2)
t.test(cases,controls)
```

Now run a Monte Carlo simulation imitating the results for the experiment for all 20 diets. If you set the seed at 100, set.seed(100), and use the same code as above inside a call to replicate how many of p-values are below 0.05?
```{r}
set.seed(100)
pvals <- replicate(20,{
  cases = rnorm(10,30,2)
  controls = rnorm(10,30,2)
  t.test(cases,controls)$p.val
})
sum(pvals < 0.05)
```

**Question 1.2.4**

Now create a simulation to learn about the distribution of the number of p-values that are less than 0.05. In question 1.2.3 we ran the 20 diet experiment once. Now we will run these 20 experiments 1,000 times and each time save the number of p-values that are less than 0.05.

Set the seed at 100 again, set.seed(100), run the code from Question 1.2.3 1,000 times, and save the number of times the p-value is less than 0.05 for each of the 1,000 instances.

What is the average of these 1,000 numbers? Note that this is the expected number of tests (out of the 20 we run) that we will reject when the null is true. 

```{r}
set.seed(100)
pvals5 <- replicate(1000,{ pvals <- replicate(20,{
                            cases = rnorm(10,30,2)
                            controls = rnorm(10,30,2)
                            t.test(cases,controls)$p.val})
                          sum(pvals < 0.05)
                }
                )
mean(pvals5)
```

**Question 1.2.5**

Note that what the answer to question 1.2.4 says is that on average, we expect some p-value to be 0.05 even when the null is true for all diets.

Using the same simulation data from question 1.2.4, for what proportion of the 1,000 replications do we reject the null hypothesis at least once (more than 0 false positives)? (Enter your response as a decimal value -- i.e. .10 for 10%.)
`r sum(pvals5> 0)/1000`
