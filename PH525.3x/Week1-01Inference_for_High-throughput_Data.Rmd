---
title: "Inference for High-throughput Data "

output: pdf_document
---
```{r}
library(rafalib)
```
## Introduction
Supposed we were given high-throughput gene expression data that was measured for several individuals in two populations. We are asked to report which genes have different average expression levels in the two populations. Note that if, instead thousands of genes, we were handed data from just one gene we could simply apply the inference techniques that we have learned before. We could, for example, use a t-test or some other test. Here we review what changes when we consider high-throughput data.

## Thousands of test
In this data we have two groups denoted with 0 and 1:
```{r}
library(GSE5859Subset)
data(GSE5859Subset)
g <- sampleInfo$group
g
```

If we were interested in a particular gene, letâ€™s arbitrarily pick the one on the 25th row, we would simply compute a t-test; assuming the data is well approximated by normal:
```{r}
e <- geneExpression[25,]
mypar2(1,2)
qqnorm(e[g==1])
qqline(e[g==1])
qqnorm(e[g==0])
qqline(e[g==0])
```

The qq-plots show that the data is well approximated by the normal approximation so apply a t-test. The t-test does not find this gene to be statistically significant:
```{r}
t.test(e[g==1],e[g==0])
```

To answer the question for each gene we simply do this for every gene. Here we will define our own function and use apply:
```{r}
myttest <- function(x) t.test(x[g==1],x[g==0],var.equal=TRUE)$p.value
pvals <- apply(geneExpression,1,myttest)
```

We can now see which genes have p-values less than, say, 0.05. For example right away we see that there are `r sum(pvals<0.05)` genes had p-values less than 0.05

However, as we will describe in more detail below, we have to be careful in interpreting this result because we have performed over 8,000 test. Note that if we performed the same procedure on random data, for which the null hypothesis is true for all feature, we obtain the following results:
```{r}
set.seed(1)
m <- nrow(geneExpression)
n <- ncol(geneExpression)
randomData <- matrix(rnorm(n*m),m,n)
nullpvals <- apply(randomData,1,myttest)
sum(nullpvals<0.05)
```

Note that 419 is roughly 0.05*8192 and we will describe the theory that tells us why this prediction works.

## Faster implementation of t-test
Before, we continue, we should note that the above implementation is very inefficient. There are several faster implementations that perform t-test for high throughput data. For example
```{r}
library(genefilter)
results <- rowttests(geneExpression,factor(g))
max(abs(pvals-results$p))
```

`genefilter` is available from the Bioconductor projects and here is how one installs it:
```
source("http://www.bioconductor.org/biocLite.R")
biocLite("genefilter")
```

## Exercise
**Question 1.2.1**

p-values are random variables.

Note that just like the sample average is a random variable because it is based on a random sample, p-values are based on random variables (sample mean, sample standard deviation) so they are also a random variable.

To see this let's see how p-values change when we take different samples. 

```{r, echo=TRUE, cache=TRUE}
set.seed(1)
library(downloader)
url = "https://raw.githubusercontent.com/genomicsclass/dagdata/master/inst/extdata/femaleControlsPopulation.csv"
filename = "femaleControlsPopulation.csv"
if (!file.exists(filename)) download(url,destfile=filename)
population = read.csv(filename)
pvals <- replicate(1000,{
  control = sample(population[,1],12)
  treatment = sample(population[,1],12)
  t.test(treatment,control)$p.val
})
head(pvals)
hist(pvals)
```

Question: What proportion of the p-values is below 0.05?
`r mean(pvals < 0.05)`

What proportion of the p-values is below 0.01?
`r mean(pvals < 0.01)`

**Question 1.2.3**

Assume you are testing the effectiveness of 20 diets on mice weight. For each of the 20 diets you run an experiment with 10 control mice and 10 treated mice. Assume the null hypothesis that the diet has no effect is true for all 20 diets and that mice weights follow a normal distribution with mean 30 grams and a standard deviation of 2 grams, run a Monte Carlo simulation for one of these studies:
```{r}
cases = rnorm(10,30,2)
controls = rnorm(10,30,2)
t.test(cases,controls)
```

Now run a Monte Carlo simulation imitating the results for the experiment for all 20 diets. If you set the seed at 100, set.seed(100), and use the same code as above inside a call to replicate how many of p-values are below 0.05?
```{r}
set.seed(100)
pvals <- replicate(20,{
  cases = rnorm(10,30,2)
  controls = rnorm(10,30,2)
  t.test(cases,controls)$p.val
})
sum(pvals < 0.05)
```

**Question 1.2.4**

Now create a simulation to learn about the distribution of the number of p-values that are less than 0.05. In question 1.2.3 we ran the 20 diet experiment once. Now we will run these 20 experiments 1,000 times and each time save the number of p-values that are less than 0.05.

Set the seed at 100 again, set.seed(100), run the code from Question 1.2.3 1,000 times, and save the number of times the p-value is less than 0.05 for each of the 1,000 instances.

What is the average of these 1,000 numbers? Note that this is the expected number of tests (out of the 20 we run) that we will reject when the null is true. 

```{r}
set.seed(100)
pvals5 <- replicate(1000,{ pvals <- replicate(20,{
                            cases = rnorm(10,30,2)
                            controls = rnorm(10,30,2)
                            t.test(cases,controls)$p.val})
                          sum(pvals < 0.05)
                }
                )
mean(pvals5)
```

**Question 1.2.5**

Note that what the answer to question 1.2.4 says is that on average, we expect some p-value to be 0.05 even when the null is true for all diets.

Using the same simulation data from question 1.2.4, for what proportion of the 1,000 replications do we reject the null hypothesis at least once (more than 0 false positives)? (Enter your response as a decimal value -- i.e. .10 for 10%.)
`r sum(pvals5> 0)/1000`

## Installing Bioconductor

Bioconductor is similar to CRAN but uses a different set of functions for downloads. It also includes many more data packages as well as annotation packages that store information about either high-throughout products or information about molecular endpoints such as genes.

## Data organized in three tables
One of the great advantages of using Bioconductor for high throughput data is that it provides object classes specifically designed to keep high throughput data organized. Below is an example of how the three tables that are needed to conduct data analysis are available from Bioconductor data objects. For example for gene expression we can use the ExpressionSet object.

```{r}
##install the Biobase package
source("http://bioconductor.org/biocLite.R")
biocLite("Biobase")
library(Biobase)
devtools::install_github("genomicsclass/GSE5859")
library(GSE5859)
data(GSE5859)
class(e)

##These objects were originally designed for gene expression data so the
##methods to extract the high throughput measurements have related names:
dat <- exprs(e)
dim(dat)

##The information about samples is also stored in this object and the
##functions to create it try to guarantee that the columns of exprs(e) 
##match the rows of the sample information table. pData is use as 
##shorthand for phenotype data. :
sampleInfo <- pData(e)
dim(sampleInfo)
head(sampleInfo)

##A final table is a table that describes the rows, in this case genes.
##Because each product will have a different table, these have already been
##created in Bioconductor. Because there are certain products that are
##widely used, Bioconductor makes databases available from which you can
##extract this information. This every object does not have to carry around
##this information:
source("http://bioconductor.org/biocLite.R")
biocLite("hgfocus.db")
library(hgfocus.db)
annot <- select(hgfocus.db, keys=featureNames(e), keytype="PROBEID", 
               columns=c("CHR", "CHRLOC", "SYMBOL"))
##pick one
annot <-annot[match(featureNames(e),annot$PROBEID),]
head(annot)
dim(annot)
```

##  Question 1.5.7

Note that this is an advanced question and that you can ask questions in the discussion forum.

Create a Monte Carlo Simulation in which you simulate measurements from 8,793 genes for 24 samples: 12 cases and 12 controls. 

```{r}
g <- factor(sampleInfo$group)
n <- 24
m <- 8793
m0 <- 8293
m1 <- 500
delta <- 2
positives <- 500
results <- matrix(0, m, 2)
set.seed(1)
fp <- replicate(1000, {
        mat <- matrix(rnorm(n*m),m,n)
    mat[1:positives,1:(n/2)] <- mat[1:positives,1:(n/2)]+delta
    library(genefilter)
    results <- rowttests(geneExpression,g)$p.value
    badj <- p.adjust(results, method = "bonferroni")
    return(sum(badj[501:m] < 0.05))
    }
    )
  
mean(fp)/m0


set.seed(1)
fn <- replicate(1000, {
    mat <- matrix(rnorm(n*m),m,n)
    mat[1:positives,1:(n/2)] <- mat[1:positives,1:(n/2)]+delta
    library(genefilter)
    results <- rowttests(geneExpression,g)$p.value
    badj <- p.adjust(results, method = "bonferroni")
    return(sum(badj[1:500] >= 0.05))
    }
    )
mean(fn)/m1
   

